{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [\n",
    "    10.0, 10.909090909090908, 11.818181818181818, 12.727272727272727,\n",
    "    13.636363636363637, 14.545454545454545, 15.454545454545453,\n",
    "    16.363636363636363, 17.272727272727273, 18.18181818181818,\n",
    "    19.09090909090909, 20.0, 20.909090909090907, 21.81818181818182,\n",
    "    22.727272727272727, 23.636363636363637, 24.545454545454547,\n",
    "    25.454545454545453, 26.363636363636363, 27.272727272727273,\n",
    "    28.18181818181818, 29.09090909090909, 30.0, 30.90909090909091,\n",
    "    31.818181818181817, 32.72727272727273, 33.63636363636364, 34.54545454545455,\n",
    "    35.45454545454545, 36.36363636363636, 37.27272727272727, 38.18181818181818,\n",
    "    39.09090909090909, 40.0, 40.90909090909091, 41.81818181818181,\n",
    "    42.72727272727273, 43.63636363636363, 44.54545454545455, 45.45454545454545,\n",
    "    46.36363636363636, 47.27272727272727, 48.18181818181818, 49.090909090909086,\n",
    "    50.0, 50.90909090909091, 51.81818181818182, 52.72727272727273,\n",
    "    53.63636363636363, 54.54545454545455, 55.45454545454545, 56.36363636363636,\n",
    "    57.27272727272727, 58.18181818181818, 59.090909090909086, 60.0,\n",
    "    60.90909090909091, 61.81818181818181, 62.72727272727273, 63.63636363636363,\n",
    "    64.54545454545455, 65.45454545454545, 66.36363636363636, 67.27272727272728,\n",
    "    68.18181818181819, 69.0909090909091, 70.0, 70.9090909090909,\n",
    "    71.81818181818181, 72.72727272727272, 73.63636363636363, 74.54545454545455,\n",
    "    75.45454545454545, 76.36363636363636, 77.27272727272727, 78.18181818181817,\n",
    "    79.0909090909091, 80.0, 80.9090909090909, 81.81818181818181,\n",
    "    82.72727272727272, 83.63636363636364, 84.54545454545455, 85.45454545454545,\n",
    "    86.36363636363636, 87.27272727272727, 88.18181818181817, 89.0909090909091,\n",
    "    90.0, 90.9090909090909, 91.81818181818181, 92.72727272727272,\n",
    "    93.63636363636364, 94.54545454545455, 95.45454545454545, 96.36363636363636,\n",
    "    97.27272727272727, 98.18181818181817, 99.0909090909091, 100.0\n",
    "  ]\n",
    "\n",
    "height = round(math.sqrt(1.05 * np.sum(requests) / 1.6), 2)\n",
    "width = round(height * 1.6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_pairs(x):\n",
    "    pairs = []\n",
    "    limit = int(abs(x) ** 0.5) + 1\n",
    "    for i in range(1, limit):\n",
    "        if x % i == 0:\n",
    "            pairs.append((i, x // i))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def calculate_piece_areas(x_cuts, y_cuts):\n",
    "    x_coords = np.sort(np.concatenate(([0], x_cuts, [width])))\n",
    "    y_coords = np.sort(np.concatenate(([0], y_cuts, [height])))\n",
    "    \n",
    "    piece_widths = np.diff(x_coords)\n",
    "    piece_heights = np.diff(y_coords)\n",
    "    \n",
    "    areas = np.concatenate(np.outer(piece_widths, piece_heights))\n",
    "    \n",
    "    return areas\n",
    "\n",
    "\n",
    "def loss_function(areas, requests):\n",
    "    R = requests\n",
    "    V = areas\n",
    "\n",
    "    num_requests = len(R)\n",
    "    num_values = len(V)\n",
    "\n",
    "    cost_matrix = np.zeros((num_requests, num_values))\n",
    "\n",
    "    for i, r in enumerate(R):\n",
    "        for j, v in enumerate(V):\n",
    "            cost_matrix[i][j] = abs(r - v) / r\n",
    "\n",
    "    row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    total_cost = sum(\n",
    "        cost_matrix[row_indices[i], col_indices[i]] for i in range(len(row_indices))\n",
    "    )\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def calculate_gradient(x_cuts, y_cuts, requests, curr_loss, epsilon=1e-3):\n",
    "    grad_x_cuts = np.zeros_like(x_cuts, dtype=float)\n",
    "    grad_y_cuts = np.zeros_like(y_cuts, dtype=float)\n",
    "    \n",
    "    for i in range(len(x_cuts)):\n",
    "        x_cuts_eps = x_cuts.copy()\n",
    "        x_cuts_eps[i] += epsilon\n",
    "        areas_eps = calculate_piece_areas(x_cuts_eps, y_cuts)\n",
    "        loss_eps = loss_function(areas_eps, requests)\n",
    "        grad_x_cuts[i] = (loss_eps - curr_loss) / epsilon\n",
    "    \n",
    "    for i in range(len(y_cuts)):\n",
    "        y_cuts_eps = y_cuts.copy()\n",
    "        y_cuts_eps[i] += epsilon\n",
    "        areas_eps = calculate_piece_areas(x_cuts, y_cuts_eps)\n",
    "        loss_eps = loss_function(areas_eps, requests)\n",
    "        grad_y_cuts[i] = (loss_eps - curr_loss) / epsilon\n",
    "    \n",
    "    return grad_x_cuts, grad_y_cuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor pair: (2, 50)\n",
      "Iteration 1: Loss = 19.474594736273943, Best loss = 19.474594736273943\n",
      "Iteration 2: Loss = 15.485760325912587, Best loss = 15.485760325912587\n",
      "Iteration 3: Loss = 12.921011694092694, Best loss = 12.921011694092694\n",
      "Iteration 4: Loss = 11.270823328932554, Best loss = 11.270823328932554\n",
      "Iteration 5: Loss = 10.13619747620583, Best loss = 10.13619747620583\n",
      "Iteration 6: Loss = 9.321909964923274, Best loss = 9.321909964923274\n",
      "Iteration 7: Loss = 9.169815890292252, Best loss = 9.169815890292252\n",
      "Iteration 8: Loss = 8.877949507788967, Best loss = 8.877949507788967\n",
      "Iteration 9: Loss = 8.206813639982323, Best loss = 8.206813639982323\n",
      "Iteration 10: Loss = 9.395932936036623, Best loss = 8.206813639982323\n",
      "Iteration 11: Loss = 9.442304360370944, Best loss = 8.206813639982323\n",
      "Iteration 12: Loss = 7.791655300700142, Best loss = 7.791655300700142\n",
      "Iteration 13: Loss = 6.453846717924384, Best loss = 6.453846717924384\n",
      "Iteration 14: Loss = 6.8929944503408445, Best loss = 6.453846717924384\n",
      "Iteration 15: Loss = 8.5210376010925, Best loss = 6.453846717924384\n",
      "Iteration 16: Loss = 7.273715870910242, Best loss = 6.453846717924384\n",
      "Iteration 17: Loss = 6.093855602620444, Best loss = 6.093855602620444\n",
      "Iteration 18: Loss = 6.262768958943061, Best loss = 6.093855602620444\n",
      "Iteration 19: Loss = 7.083265616564478, Best loss = 6.093855602620444\n",
      "Iteration 20: Loss = 7.98931526139949, Best loss = 6.093855602620444\n",
      "Iteration 21: Loss = 7.094260987336769, Best loss = 6.093855602620444\n",
      "Iteration 22: Loss = 5.8919766488162715, Best loss = 5.8919766488162715\n",
      "Iteration 23: Loss = 5.879020782280824, Best loss = 5.879020782280824\n",
      "Iteration 24: Loss = 5.180709682032764, Best loss = 5.180709682032764\n",
      "Iteration 25: Loss = 4.833831828456916, Best loss = 4.833831828456916\n",
      "Iteration 26: Loss = 4.908065864209747, Best loss = 4.833831828456916\n",
      "Iteration 27: Loss = 5.035276701010673, Best loss = 4.833831828456916\n",
      "Iteration 28: Loss = 4.3670188245351875, Best loss = 4.3670188245351875\n",
      "Iteration 29: Loss = 3.983623630286659, Best loss = 3.983623630286659\n",
      "Iteration 30: Loss = 5.164016820667979, Best loss = 3.983623630286659\n",
      "Iteration 31: Loss = 4.881931231542339, Best loss = 3.983623630286659\n",
      "Iteration 32: Loss = 4.333057052612464, Best loss = 3.983623630286659\n",
      "Iteration 33: Loss = 5.99298169172079, Best loss = 3.983623630286659\n",
      "Iteration 34: Loss = 5.490674799796223, Best loss = 3.983623630286659\n",
      "Iteration 35: Loss = 4.217100302760845, Best loss = 3.983623630286659\n",
      "Iteration 36: Loss = 4.0205173228945155, Best loss = 3.983623630286659\n",
      "Iteration 37: Loss = 3.1633802834709126, Best loss = 3.1633802834709126\n",
      "Iteration 38: Loss = 3.248865691883675, Best loss = 3.1633802834709126\n",
      "Iteration 39: Loss = 3.2247648262744444, Best loss = 3.1633802834709126\n",
      "Iteration 40: Loss = 3.0070124547817003, Best loss = 3.0070124547817003\n",
      "Iteration 41: Loss = 4.95092800557498, Best loss = 3.0070124547817003\n",
      "Iteration 42: Loss = 3.9169070257570255, Best loss = 3.0070124547817003\n",
      "Iteration 43: Loss = 3.6377162072219362, Best loss = 3.0070124547817003\n",
      "Iteration 44: Loss = 2.633957836579365, Best loss = 2.633957836579365\n",
      "Iteration 45: Loss = 4.922613283947879, Best loss = 2.633957836579365\n",
      "Iteration 46: Loss = 2.119404161875532, Best loss = 2.119404161875532\n",
      "Iteration 47: Loss = 2.6864949792866586, Best loss = 2.119404161875532\n",
      "Iteration 48: Loss = 4.823515439398085, Best loss = 2.119404161875532\n",
      "Iteration 49: Loss = 2.4548001273490128, Best loss = 2.119404161875532\n",
      "Iteration 50: Loss = 3.4196215401898575, Best loss = 2.119404161875532\n",
      "Iteration 51: Loss = 2.688202244724174, Best loss = 2.119404161875532\n",
      "Iteration 52: Loss = 4.5721079472100286, Best loss = 2.119404161875532\n",
      "Iteration 53: Loss = 5.278464633517936, Best loss = 2.119404161875532\n",
      "Iteration 54: Loss = 2.934816091637631, Best loss = 2.119404161875532\n",
      "Iteration 55: Loss = 2.2162703056480026, Best loss = 2.119404161875532\n",
      "Iteration 56: Loss = 2.0774893233335012, Best loss = 2.0774893233335012\n",
      "Iteration 57: Loss = 4.067802520429203, Best loss = 2.0774893233335012\n",
      "Iteration 58: Loss = 2.771147943292623, Best loss = 2.0774893233335012\n",
      "Iteration 59: Loss = 2.177572677013292, Best loss = 2.0774893233335012\n",
      "Iteration 60: Loss = 4.0080228926946395, Best loss = 2.0774893233335012\n",
      "Iteration 61: Loss = 2.29194322584857, Best loss = 2.0774893233335012\n",
      "Iteration 62: Loss = 2.7486444099806127, Best loss = 2.0774893233335012\n",
      "Iteration 63: Loss = 2.4724515526928155, Best loss = 2.0774893233335012\n",
      "Iteration 64: Loss = 2.5305854520114, Best loss = 2.0774893233335012\n",
      "Iteration 65: Loss = 2.292221292885092, Best loss = 2.0774893233335012\n",
      "Iteration 66: Loss = 2.0606593104817157, Best loss = 2.0606593104817157\n",
      "Iteration 67: Loss = 2.571615187911437, Best loss = 2.0606593104817157\n",
      "Iteration 68: Loss = 2.339424804303331, Best loss = 2.0606593104817157\n",
      "Iteration 69: Loss = 2.3895579855202387, Best loss = 2.0606593104817157\n",
      "Iteration 70: Loss = 4.946947589055328, Best loss = 2.0606593104817157\n",
      "Iteration 71: Loss = 2.876155247127998, Best loss = 2.0606593104817157\n",
      "Iteration 72: Loss = 4.480026971112624, Best loss = 2.0606593104817157\n",
      "Iteration 73: Loss = 2.117718619866665, Best loss = 2.0606593104817157\n",
      "Iteration 74: Loss = 4.353303639074142, Best loss = 2.0606593104817157\n",
      "Iteration 75: Loss = 4.322697662218462, Best loss = 2.0606593104817157\n",
      "Iteration 76: Loss = 2.0993272136813443, Best loss = 2.0606593104817157\n",
      "Iteration 77: Loss = 2.066036984740503, Best loss = 2.0606593104817157\n",
      "Iteration 78: Loss = 2.0202341897642446, Best loss = 2.0202341897642446\n",
      "Iteration 79: Loss = 1.8954010613647032, Best loss = 1.8954010613647032\n",
      "Iteration 80: Loss = 3.283452268526621, Best loss = 1.8954010613647032\n",
      "Iteration 81: Loss = 2.637304323753616, Best loss = 1.8954010613647032\n",
      "Iteration 82: Loss = 2.6633281396703725, Best loss = 1.8954010613647032\n",
      "Iteration 83: Loss = 1.6508876074619017, Best loss = 1.6508876074619017\n",
      "Iteration 84: Loss = 2.1738073314297606, Best loss = 1.6508876074619017\n",
      "Iteration 85: Loss = 1.8026490280096084, Best loss = 1.6508876074619017\n",
      "Iteration 86: Loss = 2.3044774387930715, Best loss = 1.6508876074619017\n",
      "Iteration 87: Loss = 2.405706476392581, Best loss = 1.6508876074619017\n",
      "Iteration 88: Loss = 1.823297676075197, Best loss = 1.6508876074619017\n",
      "Iteration 89: Loss = 1.600864503676076, Best loss = 1.600864503676076\n",
      "Iteration 90: Loss = 2.0437400182974073, Best loss = 1.600864503676076\n",
      "Iteration 91: Loss = 2.9178308639535993, Best loss = 1.600864503676076\n",
      "Iteration 92: Loss = 1.5248228352092899, Best loss = 1.5248228352092899\n",
      "Iteration 93: Loss = 1.9376619245648643, Best loss = 1.5248228352092899\n",
      "Iteration 94: Loss = 2.391705217349648, Best loss = 1.5248228352092899\n",
      "Iteration 95: Loss = 2.3851003053260986, Best loss = 1.5248228352092899\n",
      "Iteration 96: Loss = 3.002592464776258, Best loss = 1.5248228352092899\n",
      "Iteration 97: Loss = 2.0552538924550925, Best loss = 1.5248228352092899\n",
      "Iteration 98: Loss = 2.5049686665141735, Best loss = 1.5248228352092899\n",
      "Iteration 99: Loss = 4.782740431955794, Best loss = 1.5248228352092899\n",
      "Iteration 100: Loss = 1.9821315139087488, Best loss = 1.5248228352092899\n",
      "Factor pair: (4, 25)\n",
      "Iteration 1: Loss = 29.958661094256527, Best loss = 1.5248228352092899\n",
      "Iteration 2: Loss = 28.52899280650948, Best loss = 1.5248228352092899\n",
      "Iteration 3: Loss = 27.183055628469706, Best loss = 1.5248228352092899\n",
      "Iteration 4: Loss = 25.865820374033202, Best loss = 1.5248228352092899\n",
      "Iteration 5: Loss = 24.945664062964724, Best loss = 1.5248228352092899\n",
      "Iteration 6: Loss = 24.177028294363485, Best loss = 1.5248228352092899\n",
      "Iteration 7: Loss = 23.524952027274345, Best loss = 1.5248228352092899\n",
      "Iteration 8: Loss = 22.781544864278953, Best loss = 1.5248228352092899\n",
      "Iteration 9: Loss = 22.13254286547411, Best loss = 1.5248228352092899\n",
      "Iteration 10: Loss = 21.657577655463353, Best loss = 1.5248228352092899\n",
      "Iteration 11: Loss = 21.18626090599365, Best loss = 1.5248228352092899\n",
      "Iteration 12: Loss = 20.61680499091763, Best loss = 1.5248228352092899\n",
      "Iteration 13: Loss = 20.126859055308202, Best loss = 1.5248228352092899\n",
      "Iteration 14: Loss = 19.90946959346532, Best loss = 1.5248228352092899\n",
      "Iteration 15: Loss = 19.540495540652575, Best loss = 1.5248228352092899\n",
      "Iteration 16: Loss = 19.41175985103812, Best loss = 1.5248228352092899\n",
      "Iteration 17: Loss = 18.965220327672693, Best loss = 1.5248228352092899\n",
      "Iteration 18: Loss = 19.068580389093636, Best loss = 1.5248228352092899\n",
      "Iteration 19: Loss = 18.65290288635636, Best loss = 1.5248228352092899\n",
      "Iteration 20: Loss = 18.43562339746419, Best loss = 1.5248228352092899\n",
      "Iteration 21: Loss = 18.016088007801862, Best loss = 1.5248228352092899\n",
      "Iteration 22: Loss = 17.960558802161604, Best loss = 1.5248228352092899\n",
      "Iteration 23: Loss = 17.57458272122572, Best loss = 1.5248228352092899\n",
      "Iteration 24: Loss = 17.335950925645115, Best loss = 1.5248228352092899\n",
      "Iteration 25: Loss = 17.28607004464855, Best loss = 1.5248228352092899\n",
      "Iteration 26: Loss = 16.86704187261643, Best loss = 1.5248228352092899\n",
      "Iteration 27: Loss = 16.96255588575205, Best loss = 1.5248228352092899\n",
      "Iteration 28: Loss = 16.693041513511798, Best loss = 1.5248228352092899\n",
      "Iteration 29: Loss = 16.472463479247438, Best loss = 1.5248228352092899\n",
      "Iteration 30: Loss = 16.49353911784409, Best loss = 1.5248228352092899\n",
      "Iteration 31: Loss = 16.357960070501736, Best loss = 1.5248228352092899\n",
      "Iteration 32: Loss = 16.45061820554914, Best loss = 1.5248228352092899\n",
      "Iteration 33: Loss = 16.295938095135753, Best loss = 1.5248228352092899\n",
      "Iteration 34: Loss = 15.957307532092276, Best loss = 1.5248228352092899\n",
      "Iteration 35: Loss = 15.868612550860146, Best loss = 1.5248228352092899\n",
      "Iteration 36: Loss = 15.6001167283019, Best loss = 1.5248228352092899\n",
      "Iteration 37: Loss = 15.547599426684997, Best loss = 1.5248228352092899\n",
      "Iteration 38: Loss = 15.482570575829598, Best loss = 1.5248228352092899\n",
      "Iteration 39: Loss = 15.408124742084627, Best loss = 1.5248228352092899\n",
      "Iteration 40: Loss = 15.243966343667099, Best loss = 1.5248228352092899\n",
      "Iteration 41: Loss = 15.235698440842413, Best loss = 1.5248228352092899\n",
      "Iteration 42: Loss = 15.014048978993637, Best loss = 1.5248228352092899\n",
      "Iteration 43: Loss = 15.014603713751082, Best loss = 1.5248228352092899\n",
      "Iteration 44: Loss = 15.153031041579961, Best loss = 1.5248228352092899\n",
      "Iteration 45: Loss = 14.909286074915245, Best loss = 1.5248228352092899\n",
      "Iteration 46: Loss = 14.559698257183921, Best loss = 1.5248228352092899\n",
      "Iteration 47: Loss = 14.548553979094025, Best loss = 1.5248228352092899\n",
      "Iteration 48: Loss = 14.381715599973308, Best loss = 1.5248228352092899\n",
      "Iteration 49: Loss = 14.498666785604687, Best loss = 1.5248228352092899\n",
      "Iteration 50: Loss = 14.397257624295442, Best loss = 1.5248228352092899\n",
      "Iteration 51: Loss = 14.41728081919622, Best loss = 1.5248228352092899\n",
      "Iteration 52: Loss = 14.125015091816083, Best loss = 1.5248228352092899\n",
      "Iteration 53: Loss = 14.005041242944078, Best loss = 1.5248228352092899\n",
      "Iteration 54: Loss = 13.930389681454274, Best loss = 1.5248228352092899\n",
      "Iteration 55: Loss = 13.76737151239827, Best loss = 1.5248228352092899\n",
      "Iteration 56: Loss = 13.81881543136578, Best loss = 1.5248228352092899\n",
      "Iteration 57: Loss = 13.727026389470735, Best loss = 1.5248228352092899\n",
      "Iteration 58: Loss = 13.636264189806615, Best loss = 1.5248228352092899\n",
      "Iteration 59: Loss = 13.612843188603355, Best loss = 1.5248228352092899\n",
      "Iteration 60: Loss = 13.553843605309588, Best loss = 1.5248228352092899\n",
      "Iteration 61: Loss = 13.678973955016895, Best loss = 1.5248228352092899\n",
      "Iteration 62: Loss = 13.479627092988705, Best loss = 1.5248228352092899\n",
      "Iteration 63: Loss = 13.353309711503414, Best loss = 1.5248228352092899\n",
      "Iteration 64: Loss = 13.075604955635146, Best loss = 1.5248228352092899\n",
      "Iteration 65: Loss = 13.367815629194208, Best loss = 1.5248228352092899\n",
      "Iteration 66: Loss = 13.045375892842332, Best loss = 1.5248228352092899\n",
      "Iteration 67: Loss = 13.10716131322281, Best loss = 1.5248228352092899\n",
      "Iteration 68: Loss = 13.006983549635601, Best loss = 1.5248228352092899\n",
      "Iteration 69: Loss = 13.070975522140904, Best loss = 1.5248228352092899\n",
      "Iteration 70: Loss = 13.111921412772006, Best loss = 1.5248228352092899\n",
      "Iteration 71: Loss = 12.554870055521555, Best loss = 1.5248228352092899\n",
      "Iteration 72: Loss = 12.812016216558249, Best loss = 1.5248228352092899\n",
      "Iteration 73: Loss = 12.754007716036844, Best loss = 1.5248228352092899\n",
      "Iteration 74: Loss = 12.695943244274474, Best loss = 1.5248228352092899\n",
      "Iteration 75: Loss = 12.740245117669508, Best loss = 1.5248228352092899\n",
      "Iteration 76: Loss = 12.751263476343965, Best loss = 1.5248228352092899\n",
      "Iteration 77: Loss = 12.457305671592293, Best loss = 1.5248228352092899\n",
      "Iteration 78: Loss = 12.745481705544336, Best loss = 1.5248228352092899\n",
      "Iteration 79: Loss = 12.338234582493651, Best loss = 1.5248228352092899\n",
      "Iteration 80: Loss = 12.386771362076693, Best loss = 1.5248228352092899\n",
      "Iteration 81: Loss = 12.222825184340262, Best loss = 1.5248228352092899\n",
      "Iteration 82: Loss = 12.454053040740236, Best loss = 1.5248228352092899\n",
      "Iteration 83: Loss = 12.480375990625195, Best loss = 1.5248228352092899\n",
      "Iteration 84: Loss = 12.243819569829586, Best loss = 1.5248228352092899\n",
      "Iteration 85: Loss = 12.128432319622362, Best loss = 1.5248228352092899\n",
      "Iteration 86: Loss = 12.25331376999991, Best loss = 1.5248228352092899\n",
      "Iteration 87: Loss = 11.98150110518646, Best loss = 1.5248228352092899\n",
      "Iteration 88: Loss = 11.847075383349372, Best loss = 1.5248228352092899\n",
      "Iteration 89: Loss = 12.041005801125282, Best loss = 1.5248228352092899\n",
      "Iteration 90: Loss = 12.5006055749513, Best loss = 1.5248228352092899\n",
      "Iteration 91: Loss = 12.384507577396564, Best loss = 1.5248228352092899\n",
      "Iteration 92: Loss = 11.566918867303523, Best loss = 1.5248228352092899\n",
      "Iteration 93: Loss = 11.905828818230674, Best loss = 1.5248228352092899\n",
      "Iteration 94: Loss = 11.680745402151551, Best loss = 1.5248228352092899\n",
      "Iteration 95: Loss = 11.713265062388011, Best loss = 1.5248228352092899\n",
      "Iteration 96: Loss = 11.535219362632002, Best loss = 1.5248228352092899\n",
      "Iteration 97: Loss = 11.814816560044525, Best loss = 1.5248228352092899\n",
      "Iteration 98: Loss = 11.37464976672764, Best loss = 1.5248228352092899\n",
      "Iteration 99: Loss = 11.68788509918092, Best loss = 1.5248228352092899\n",
      "Iteration 100: Loss = 11.6260688462363, Best loss = 1.5248228352092899\n",
      "Factor pair: (5, 20)\n",
      "Iteration 1: Loss = 29.61083068378824, Best loss = 1.5248228352092899\n",
      "Iteration 2: Loss = 27.191675380101223, Best loss = 1.5248228352092899\n",
      "Iteration 3: Loss = 25.940901331467927, Best loss = 1.5248228352092899\n",
      "Iteration 4: Loss = 24.663776675191123, Best loss = 1.5248228352092899\n",
      "Iteration 5: Loss = 22.71012267425417, Best loss = 1.5248228352092899\n",
      "Iteration 6: Loss = 21.547250492858268, Best loss = 1.5248228352092899\n",
      "Iteration 7: Loss = 20.81898174039214, Best loss = 1.5248228352092899\n",
      "Iteration 8: Loss = 19.694433524151687, Best loss = 1.5248228352092899\n",
      "Iteration 9: Loss = 18.516051751894043, Best loss = 1.5248228352092899\n",
      "Iteration 10: Loss = 17.660362421363942, Best loss = 1.5248228352092899\n",
      "Iteration 11: Loss = 17.242277865254252, Best loss = 1.5248228352092899\n",
      "Iteration 12: Loss = 16.41302411801818, Best loss = 1.5248228352092899\n",
      "Iteration 13: Loss = 15.926630241947638, Best loss = 1.5248228352092899\n",
      "Iteration 14: Loss = 15.707451701884173, Best loss = 1.5248228352092899\n",
      "Iteration 15: Loss = 14.969949850375356, Best loss = 1.5248228352092899\n",
      "Iteration 16: Loss = 14.075748536895658, Best loss = 1.5248228352092899\n",
      "Iteration 17: Loss = 13.215963920505237, Best loss = 1.5248228352092899\n",
      "Iteration 18: Loss = 12.739264863580953, Best loss = 1.5248228352092899\n",
      "Iteration 19: Loss = 12.033778598924098, Best loss = 1.5248228352092899\n",
      "Iteration 20: Loss = 11.520564149203116, Best loss = 1.5248228352092899\n",
      "Iteration 21: Loss = 11.166641928633348, Best loss = 1.5248228352092899\n",
      "Iteration 22: Loss = 10.549418679843548, Best loss = 1.5248228352092899\n",
      "Iteration 23: Loss = 10.411091864334093, Best loss = 1.5248228352092899\n",
      "Iteration 24: Loss = 10.026928551948373, Best loss = 1.5248228352092899\n",
      "Iteration 25: Loss = 9.722792730364286, Best loss = 1.5248228352092899\n",
      "Iteration 26: Loss = 9.277169426660944, Best loss = 1.5248228352092899\n",
      "Iteration 27: Loss = 9.053820786288385, Best loss = 1.5248228352092899\n",
      "Iteration 28: Loss = 9.131287192709683, Best loss = 1.5248228352092899\n",
      "Iteration 29: Loss = 8.51706056548323, Best loss = 1.5248228352092899\n",
      "Iteration 30: Loss = 8.692010766068991, Best loss = 1.5248228352092899\n",
      "Iteration 31: Loss = 8.125470872702826, Best loss = 1.5248228352092899\n",
      "Iteration 32: Loss = 8.291952939754783, Best loss = 1.5248228352092899\n",
      "Iteration 33: Loss = 7.8206138159668, Best loss = 1.5248228352092899\n",
      "Iteration 34: Loss = 7.927747863524714, Best loss = 1.5248228352092899\n",
      "Iteration 35: Loss = 7.605411251640821, Best loss = 1.5248228352092899\n",
      "Iteration 36: Loss = 7.083979988605556, Best loss = 1.5248228352092899\n",
      "Iteration 37: Loss = 6.8347657739524195, Best loss = 1.5248228352092899\n",
      "Iteration 38: Loss = 6.806781229359429, Best loss = 1.5248228352092899\n",
      "Iteration 39: Loss = 6.717394399187383, Best loss = 1.5248228352092899\n",
      "Iteration 40: Loss = 6.867942829235299, Best loss = 1.5248228352092899\n",
      "Iteration 41: Loss = 6.140930744983632, Best loss = 1.5248228352092899\n",
      "Iteration 42: Loss = 6.3848639020677735, Best loss = 1.5248228352092899\n",
      "Iteration 43: Loss = 5.866911242438621, Best loss = 1.5248228352092899\n",
      "Iteration 44: Loss = 5.600533214594131, Best loss = 1.5248228352092899\n",
      "Iteration 45: Loss = 5.328430304862384, Best loss = 1.5248228352092899\n",
      "Iteration 46: Loss = 4.877782029306013, Best loss = 1.5248228352092899\n",
      "Iteration 47: Loss = 4.8573854550162014, Best loss = 1.5248228352092899\n",
      "Iteration 48: Loss = 5.396485725785227, Best loss = 1.5248228352092899\n",
      "Iteration 49: Loss = 4.735126143994296, Best loss = 1.5248228352092899\n",
      "Iteration 50: Loss = 4.414696919073499, Best loss = 1.5248228352092899\n",
      "Iteration 51: Loss = 4.353001161605162, Best loss = 1.5248228352092899\n",
      "Iteration 52: Loss = 4.7093199644671495, Best loss = 1.5248228352092899\n",
      "Iteration 53: Loss = 4.294972384496972, Best loss = 1.5248228352092899\n",
      "Iteration 54: Loss = 4.309663789306317, Best loss = 1.5248228352092899\n",
      "Iteration 55: Loss = 3.963057995743681, Best loss = 1.5248228352092899\n",
      "Iteration 56: Loss = 3.6782100625137417, Best loss = 1.5248228352092899\n",
      "Iteration 57: Loss = 3.7972175104147508, Best loss = 1.5248228352092899\n",
      "Iteration 58: Loss = 3.7119812741572575, Best loss = 1.5248228352092899\n",
      "Iteration 59: Loss = 3.4522700027736617, Best loss = 1.5248228352092899\n",
      "Iteration 60: Loss = 3.3875502415827086, Best loss = 1.5248228352092899\n",
      "Iteration 61: Loss = 3.048913121427855, Best loss = 1.5248228352092899\n",
      "Iteration 62: Loss = 3.4434080299609935, Best loss = 1.5248228352092899\n",
      "Iteration 63: Loss = 2.3982948123056382, Best loss = 1.5248228352092899\n",
      "Iteration 64: Loss = 2.587179048124824, Best loss = 1.5248228352092899\n",
      "Iteration 65: Loss = 2.333569571402469, Best loss = 1.5248228352092899\n",
      "Iteration 66: Loss = 2.973028369045198, Best loss = 1.5248228352092899\n",
      "Iteration 67: Loss = 2.3674178989542765, Best loss = 1.5248228352092899\n",
      "Iteration 68: Loss = 2.386694122238514, Best loss = 1.5248228352092899\n",
      "Iteration 69: Loss = 2.1441588271918803, Best loss = 1.5248228352092899\n",
      "Iteration 70: Loss = 2.531167425191365, Best loss = 1.5248228352092899\n",
      "Iteration 71: Loss = 2.4266158787585526, Best loss = 1.5248228352092899\n",
      "Iteration 72: Loss = 2.7128226732404324, Best loss = 1.5248228352092899\n",
      "Iteration 73: Loss = 2.364441691228974, Best loss = 1.5248228352092899\n",
      "Iteration 74: Loss = 2.3902544491935194, Best loss = 1.5248228352092899\n",
      "Iteration 75: Loss = 2.30504296079431, Best loss = 1.5248228352092899\n",
      "Iteration 76: Loss = 2.392977182339154, Best loss = 1.5248228352092899\n",
      "Iteration 77: Loss = 2.0985781004367654, Best loss = 1.5248228352092899\n",
      "Iteration 78: Loss = 2.52598312690746, Best loss = 1.5248228352092899\n",
      "Iteration 79: Loss = 2.1388197407929916, Best loss = 1.5248228352092899\n",
      "Iteration 80: Loss = 2.420303743055401, Best loss = 1.5248228352092899\n",
      "Iteration 81: Loss = 1.9162807341654924, Best loss = 1.5248228352092899\n",
      "Iteration 82: Loss = 1.7116542336847729, Best loss = 1.5248228352092899\n",
      "Iteration 83: Loss = 1.7482870587977004, Best loss = 1.5248228352092899\n",
      "Iteration 84: Loss = 1.832674705393276, Best loss = 1.5248228352092899\n",
      "Iteration 85: Loss = 1.9649366602511722, Best loss = 1.5248228352092899\n",
      "Iteration 86: Loss = 2.3464063011513514, Best loss = 1.5248228352092899\n",
      "Iteration 87: Loss = 2.2916018940375604, Best loss = 1.5248228352092899\n",
      "Iteration 88: Loss = 1.6469629305876465, Best loss = 1.5248228352092899\n",
      "Iteration 89: Loss = 2.049587648609208, Best loss = 1.5248228352092899\n",
      "Iteration 90: Loss = 2.0021292560778634, Best loss = 1.5248228352092899\n",
      "Iteration 91: Loss = 1.800314593425769, Best loss = 1.5248228352092899\n",
      "Iteration 92: Loss = 1.9020053635348864, Best loss = 1.5248228352092899\n",
      "Iteration 93: Loss = 1.88351148558084, Best loss = 1.5248228352092899\n",
      "Iteration 94: Loss = 2.2491150884028874, Best loss = 1.5248228352092899\n",
      "Iteration 95: Loss = 2.1000555655950905, Best loss = 1.5248228352092899\n",
      "Iteration 96: Loss = 1.8965441480725556, Best loss = 1.5248228352092899\n",
      "Iteration 97: Loss = 1.782231483174578, Best loss = 1.5248228352092899\n",
      "Iteration 98: Loss = 1.7909488125034392, Best loss = 1.5248228352092899\n",
      "Iteration 99: Loss = 1.8004253364215503, Best loss = 1.5248228352092899\n",
      "Iteration 100: Loss = 1.4761249646573111, Best loss = 1.4761249646573111\n",
      "Best loss: 1.4761249646573111\n"
     ]
    }
   ],
   "source": [
    "factors = factor_pairs(len(requests))[1:-1]\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_x_cuts = None\n",
    "best_y_cuts = None\n",
    "for factor in factors:\n",
    "    print(f\"Factor pair: {factor}\")\n",
    "    num_horizontal, num_vertical = factor\n",
    "\n",
    "    x_cuts = np.array(np.random.randint(1, width, num_vertical), dtype=float)\n",
    "    y_cuts = np.array(np.random.randint(1, height, num_horizontal), dtype=float)\n",
    "\n",
    "    best_x_cuts = x_cuts.copy()\n",
    "    best_y_cuts = y_cuts.copy()\n",
    "\n",
    "    learning_rate = 0.1\n",
    "    num_iterations = 100\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        areas = calculate_piece_areas(x_cuts, y_cuts)\n",
    "        loss = loss_function(areas, requests)\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_x_cuts = x_cuts.copy()\n",
    "            best_y_cuts = y_cuts.copy()\n",
    "        \n",
    "        grad_x_cuts, grad_y_cuts = calculate_gradient(x_cuts, y_cuts, requests, loss)\n",
    "        \n",
    "        x_cuts -= learning_rate * grad_x_cuts\n",
    "        y_cuts -= learning_rate * grad_y_cuts\n",
    "        print(f'Iteration {i + 1}: Loss = {loss}, Best loss = {best_loss}')\n",
    "print(\"Best loss:\", best_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
